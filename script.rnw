\documentclass{article}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{subcaption}


\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}
\graphicspath{ {Assets/} }
\begin{document}


\begin{titlepage}
    \begin{center}
        \vspace*{1cm}
        
        \textbf{Analyzing the Global Terrorism Database}
        
        \vspace{0.5cm}
        Viewing Terrorism Globally and in the United Kingdom
        
        \vspace{1.5cm}
        
        \textbf{Liam Cottrell - 1302859}
        
        \vfill
        
        
        \vspace{0.8cm}
        
        \includegraphics[width=1\textwidth]{coverImage}
        \vspace{0.4cm}
        {\scriptsize Wordcloud Generated from most frequently used words when Reports summarize Terrorism}\\
        \vspace{0.4cm}
        CM3111 - Full Time: Big Data Analytics\\
        Robert Gordon University\\
        December 18th, 2017
        
    \end{center}
\end{titlepage}


\tableofcontents
\listoffigures

\pagebreak


\section{Introduction}
\label{sec:Introduction}

  \subsection{Terrorism}
  \label{ssec:Terrorism}
  
    For the last decade, Terrorist Incidents have been perceived to be on the rise. Every time you turn on the TV and watch the evening news, you are presented with a new headline from a new city that has been struck by a horrific event. What this Big Data module has taught me is that the art of analyzing and visualizing data is an act of telling a story to a viewer. My aim for this report, is to see if the stories that we are told, about the state of Global Terrorism, are as damning as we are led to believe.\par If this is the case, who are the main players? Do groups perform terrorism on a global scale? What does terrorism look like, and can we predict who does it?
    
    \vspace{4cm}
  
  \subsection{Citing Sources}
  \label{ssec:Citing Source}
    Throughout my report, I will make use of the following data set which I acquired from \href{https://www.kaggle.com/START-UMD/gtd}{kaggle}.\par National Consortium for the Study of Terrorism and Responses to Terrorism (START). (2017). Global Terrorism Database [Data file]. Retrieved from \href{https://www.start.umd.edu/gtd}{The Global Terrorism Database's (GTD's) website}.
    
\pagebreak

\section{Environment Setup}
\label{sec:Enviroment Setup}

  \subsection{Downloading Dataset}
  \label{ssec:Downloading}
    We will need to download our dataset from the following link: \href{https://www.kaggle.com/START-UMD/gtd}{here}, and make sure to place it in our projects "Asset" folder. We will also need to rename the dataset to following name "globalterrorismdb.csv" so that we can import it correctly into our script.

    <<Kniter suppress warning messages, eval=TRUE,echo=FALSE>>=
      library(knitr)
      knitr::opts_chunk$set(warning=FALSE, message=FALSE)
      knitr::opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
      options(stringsAsFactors = FALSE)
    @
    
  \subsection{Include Libraries}
  \label{ssec:Include Libraries}
  
  We will start our script with library declarations of all the required packages. These packages are vital to the operation of the script. There are some very nice visualization packages such as "ggplot2" and "wordcloud2" that we will get to see more of, throughout the report. 
  
    <<List of packages that will be utilised in my application, eval=TRUE, echo=TRUE>>=
      library(tidyverse)
      library(ggplot2)
      library(ggmap)
      library(maps)
      library(mapdata)
      library(maptools)
      library(ggthemes)
      library(ggalt)
      library(wordcloud)
      library(tm)
      library(RColorBrewer)
      library(SnowballC)
      library(wordcloud2)
      library(Matrix)
      library(tidytext)
      library(magrittr)
      library(webshot)
      library(ddR)
      library(randomForest.ddR)
      library(caret)
      library(forcats)
      library(plyr)
      library(shiny)
      library(trelliscopejs)
      library(htmlwidgets)
      library(grid)
      library(png)
      library(xtable)
    @
    
  \pagebreak
  
  \subsection{Create Seed}
  \label{ssec:CreateSeed}
  
    We will set the seed of the script to a specific number so that our results can be repeatable and consistent.
    
    <<Setting Document Seed for repetable results, echo=TRUE, eval=FALSE>>=
      set.seed(4205426)
    @
    
  \subsection{Set Core Count}
  \label{ssec:CoreCount}
    As part of my script, I've used parallel processing and used the R Package drandomForest. This package is similar in functionality to the standard ranomForest package however, it works in a distributed fashion. It will create many instances of R to perform calculations in parallel, speeding up the completion time of generating my Random Forest, later in this document. If you want to learn more about the drandomForest Package, read \href{https://www.rdocumentation.org/packages/randomForest.ddR/versions/0.1.1/topics/drandomForest}{here}.\par I'm running my Script off a Server I have rented which has a Xeon E5-1650 v4 6 core processor with 12 logical processors. You will need to change the following value to the amount of logical processors your computer has, which you can find out online.\par We will set the following nExecutor parameter to 12, but your setup may be different.
    <<Set Core quanity of host machine to perfom parallel processing, echo=TRUE, eval=TRUE>>=
      nExecutor <- 12
    @
    

    <<Watermark Creation, eval=TRUE, echo=FALSE>>=
      img <- png::readPNG("Assets/watermark.png")
      watermark <- matrix(rgb(img[,,1],img[,,2],img[,,3], img[,,4] * 0.1), nrow=dim(img)[1]) #0.1 is alpha
    @



  \subsection{Import Dataset}
  \label{ssec:ImportDataset}
    We will import our csv file in the following few lines of code:
    
    \small
      <<Import dataset from csv file, eval=TRUE, echo=TRUE>>=
        # Import csv file from the Assets folder
        orig_df = read.csv("Assets/globalterrorismdb.csv",
                           na.strings=c("", "NA", "NULL"))  
        my_df <- orig_df
        attach(my_df)
      @
    \normalsize
    
    Once dataset has been imported and assigned, create a working copy that we can use in our application under the dataframe mydf. We will leave origdf alone as a backup as it is always good practice to do so!
    
    
    \pagebreak
    
    
\section{Data Exploration}
\label{sec:DataExploration}

  \subsection{Size of GTD}
  \label{ssec:SizeGTD}
  
    Start by seeing how many rows and feature our dataset has by using the dim function in R to see the dataframes dimensions.
    
    <<Data Exploration, eval=TRUE,echo=TRUE>>=
      dim(my_df)
    @
    The Dataset has 170350 rows and 135 features. We can see the names of these features by making use of the names function in R as follows:
    
    \small
    <<Feature Names, eval=TRUE,echo=TRUE>>=
      names(my_df)
    @
    \normalsize
    
    \clearpage
    \pagebreak
    
\section{Data Analysis}
\label{sec:Data Analysis}

  \subsection{Spread of Incidents over the years}
  \label{ssec:SpreadIncidentsYears}
  
    \subsubsection{What we are looking for}
    
      The first plot we are going to make is to see what the spread of incidents is like over the years. The GTD supplies information of incidents that were recorded from 1970 all the way to last year, 2017. What we would be interested in seeing is if terrorism is on the incline or is the rate of terrorist activity at a consistent rate.
      
    \subsubsection{Using R to Create our Plot}
      We will make use of the powerful ggplot2 package to create a histogram of all incidents from 1970 to 2016.\par We shall declare a variable and start adding aspect to our plot such as Title, theme and x and y labels.
      \small
        <<Histogram of Recorded Terrorist Incidents, eval=TRUE, echo=TRUE>>=
          my_df.Histogram.Year <- ggplot(my_df, aes(x = iyear)) 
          my_df.Histogram.Year <- my_df.Histogram.Year + 
            geom_bar()
          my_df.Histogram.Year <- my_df.Histogram.Year + 
            theme_igray()
          my_df.Histogram.Year <- my_df.Histogram.Year + 
            scale_colour_tableau()
          my_df.Histogram.Year <- my_df.Histogram.Year + 
            ggtitle("Histogram of Recorded Terrorist Incidents")
          my_df.Histogram.Year <- my_df.Histogram.Year + 
            xlab("Year (Year)")
          my_df.Histogram.Year <- my_df.Histogram.Year + 
            ylab("Quantity of Incidents (incidents)")
        @
      \normalsize
      
      <<Add Watermark to Year Histogram, eval=TRUE, echo=FALSE>>=
        my_df.Histogram.Year <- my_df.Histogram.Year + 
            annotation_custom(xmin=-Inf,
                              ymin=-Inf,
                              xmax=Inf,
                              ymax=Inf,
                              rasterGrob(
                                watermark
                                )
                              )
      @
      
      \pagebreak
      
      \begin{figure}
        <<Plot Histogram of Recorded Terrorist Incidents, eval=TRUE, echo=FALSE>>=
          my_df.Histogram.Year
        @
        \begin{center}
          \caption{Histogram: Frequency of Terrorist Incidents over Years}
        \end{center}
        \label{fig:HistYear}
      \end{figure}
      
      \clearpage
      
      
      As we can see from \ref{fig:HistYear}, there has been a dramatic increase of reported terrorist incidents from 2013 onward. Looking at the data, it shows evidence it shows terrorist incidents having a decline as the new millennium started, then dramatically increasing over the next decade.\par This information is interesting and we will continue to visualize incidents from this set
    

  \subsection{Map of All Terrorist attacks}
  \label{ssec:MapTerroristAttacks}
  
    \subsubsection{Visualizing location data}
    
      Our Dataset has longitude and latitude values for some incidents, so it would be nice to visualize this information on a map. This will allow us to see if there are any large areas of concentrated incidents in the world, and allow us to explore certain regions or countries further to see what sort of terrorist activity is going on. 
      
    \subsubsection{Using R to Create our Map}
      
      We will, once again, use ggplot2 to plot our map. We will start by getting vector information from the package mapdata that we can map the coordinates of to a ggplot geom polygon. This will gives us a scale-able image of the world, but due to the way the globe is distorted, we will have to compensate for its curvature by multiplying the map positions by 1.3. Once we have done this, we can then plot points on the map where every terrorist incident occurs, and after theme information, we should get the below results.

      \small
      <<Map of all Terrorist attacks, eval=TRUE, echo=TRUE>>=
        worldmapcords <- map_data("world")
        worldmapcords <- worldmapcords[worldmapcords$region != "Antarctica",]
        worldmap <- ggplot() 
        worldmap <- worldmap + 
          geom_polygon(data = worldmapcords,
                                            aes(
                                              x = long,
                                              y = lat,
                                              group = group
                                              ),
                                            fill="#7f7f7f",
                                            size = 0.05,
                                            alpha = 1/3)
        worldmap <- worldmap + 
          coord_fixed(1.3)
        worldmap <- worldmap + 
          geom_point(data = my_df,
                     aes(x = longitude,
                         y = latitude),
                     color="#991500",
                     size = 0.10,
                     alpha = 1/30) 
        
        worldmap <- worldmap + 
          scale_color_tableau()
        worldmap <- worldmap + 
          theme_map()
        worldmap <- worldmap + 
          theme(strip.background=element_blank())
        worldmap <- worldmap + 
          theme(legend.position="none")
        worldmap <- worldmap + 
          ggtitle("Map of All Terrorist Activity, 1970-2016")
      @
      \normalsize
    
      \pagebreak
      
      \begin{figure}
        <<Plot Map of all Terrorist attacks, eval=TRUE, echo=FALSE>>=
          worldmap
        @
        \begin{center}
          \caption{Map: Latitude/Longitude coords mapped to global map, using ggplot2}
        \end{center}
        \label{fig:worldmap}
      \end{figure}
      
      \clearpage
      
      \pagebreak
      
      You can see in \ref{fig:worldmap}, there is a surprisingly high concentration of terrorist attacks that happen in the United Kingdom. We shall explore the terrorist groups that have committed attacks in the United Kingdom as a result of these findings.
    

    
      <<Detach mydf, eval=TRUE, echo=FALSE>>=
        detach(my_df)
      @
    
  \subsection{Pie-chart of Frequency of Global Terrorist attack types}
  \label{ssec:PieChartAttackType}
  
    \subsubsection{Introduction}
      We will take a small selection of data from our main dataset, to create a new dataframe. We wish to find out the frequency of different Attack Types to see if there are particular Attack Types that are more persistent.
  
    \subsubsection{Sub-setting Attack data}
      
      We will use the count function that comes with the plyr Package to get a quick frequency dataframe of all the attack types.\par From there, we will calculate the sum of the total quantity of attack types that we will use to calculate percentage later. 
    
      <<Create AttackType Dataframe, eval=TRUE, echo=TRUE>>=
        my_df.countattacktype <- count(my_df$attacktype1_txt)
        my_df.countattacktype.percent <- sum(my_df.countattacktype$freq)
      @
      
      
    \subsubsection{Calculate Attack Type Percentage and Sort}
      We will now append a new column called percent to our dataframe that will contain the percentage of specific attack type in proportion to its peers. To  do this, will will need to create a loop to access the specific index of the record we wish to change and the index of percent we wish to create. From there its a case of calculating the percentage with the rest of the information that we acquired before.   
      \small
      <<Calculate Frequency and Percentage of AttackTypes, eval=TRUE, echo=TRUE>>=
        
        for(i in 1:nrow(my_df.countattacktype)){
          my_df.countattacktype$percent[i] <- (my_df.countattacktype$freq[i]/
                                                 my_df.countattacktype.percent)*100
        }
        my_df.countattacktype <- my_df.countattacktype[order(my_df.countattacktype$freq,
                                                                  decreasing = T),]
      @
      \normalsize
      Now we have populated the percentage field, we can sort our dataframe from highest value of frequency to lowest which will make the data nice to read when we create a table with it next.
    \subsubsection{Attack Type Table}
      We will make use of the xtable package, that takes in our dataframe as a parameter, and outputs it to my document in \LaTeX  formatting. 
      <<AttackType Dataframe to table, eval=TRUE, echo=FALSE, results="asis">>=
       print(xtable(my_df.countattacktype,
                    caption="Frequency and Percentage of Attack Types"))
      @
      We can see from this table that Bombing/Explosion seems to have the highest percentage of frequency vs the alternative Attack Types.
    \subsubsection{Plot table to Pie Chart}
      We can take the information in our dataframe and create a Pie Chart to better visualize it. We can make use of ggplot2 to create an initial bar plot, then convert it into a circular version, aka a pie chart. 
      <<BarChart of all Terrorist attack types, eval=TRUE, echo=TRUE>>=
      my_df.countattacktype.bp <- ggplot(my_df.countattacktype,
                                         aes(x="",
                                             y=percent,
                                             fill=x)
                                         )
      my_df.countattacktype.bp <- my_df.countattacktype.bp +
        geom_bar(width = 1,
                 stat = "identity")
      my_df.countattacktype.bp <- my_df.countattacktype.bp +
        scale_fill_brewer(palette="Spectral")
      my_df.countattacktype.bp <- my_df.countattacktype.bp +
        ggtitle("Pie Chart of Proportion of Terrorist Attack types, 1970-2016")
      my_df.countattacktype.bp <- my_df.countattacktype.bp +
        labs(fill = "Attack Types")
      my_df.countattacktype.bp <- my_df.countattacktype.bp +
        xlab("")
      my_df.countattacktype.bp <- my_df.countattacktype.bp +
        ylab("")
      @
      Once we have our Bar chart created, we can now go about converting it to a Pie chart.
      
      <<Convert to Piechart of all Terrorist attack types, eval=TRUE, echo=TRUE>>=
        my_df.countattacktype.pie <- my_df.countattacktype.bp +
          coord_polar("y",
                      start=0)
      @
      
      
      \pagebreak
      
      \begin{figure}
        <<Plot Piechart of all Terrorist attack types, eval=TRUE, echo=FALSE>>=
          my_df.countattacktype.pie
        @
        \begin{center}
          \caption{Pie Chart: Percentage distribution of Attack type frequency information, using ggplot2}
        \end{center}
        \label{fig:AttackPie}
      \end{figure}
      
      \clearpage
      The pie chart shows the same information that our table did from before. I find, however, it better visualizes the surprising proportion of terrorist attacks that are Bombing/Explosion conducted. The proportion of the next closest type of attack,Armed Assault, is significantly smaller in comparison.
      
      \pagebreak
      
      
      
  \subsection{Corpus's, DocumentTextMatrix's and Wordcloud's}
  \label{ssec:CorpusDTMWordcloud}
  
    \subsubsection{Section Introduction}
      Another form of visualizing data for analysis is to use the wordcloud, or wordcloud2 packages. These packages both generate an image of multiple words, their size dictating the higher the frequency of use. The GTD supplies us with a feature called Summary which contains summary information about specific terrorist incidents. What would be interesting to see is the most commonly used words in theses passages, and see if the most frequent words used in summaries globally differs from a specific country, such as the United Kingdom.
  
    \subsubsection{Corpus Creation Function}
      To be able to create a Document Text Matrix, we must be able to collate a collection of text documents. A Corpus, or VCorpus, allows us to be able to do just that. We will make use of the very powerful tm, or Text Mining, Package in R that can help us in the creation of Corpus's and then later, Document Text Matrix's.\par We will create a function that will allow us to reduce the repetition of code, as we plan to make more than one wordcloud. The purpose of our buildCopus function is to take in a list of summary records, then convert them to a Corpus to be sterilized. We can sterilize the documents for symbols, via the use of regular expressions, and stop words such as, as and the that, etc... We will also make sure to remove white space and numbers from our Corpus's as to only retain words of particular interest, such as verbs and nouns.

      <<Corpus Building Function Used throughout program, eval=TRUE, echo=TRUE>>=
        buildCorpus <- function(someText){
          # build a corpus, and specify the source to be character vectors
          myCorpus <- Corpus(VectorSource(someText))
          # I had to add this line to make the code work
          # For windows, it may not be an issue
          myCorpus <- tm_map(myCorpus,
                                      content_transformer(function(x) iconv(x,
                                                                            to='UTF-8',
                                                                            sub='byte')))
          myCorpus <- tm_map(myCorpus,
                             content_transformer(tolower))
          # remove punctuation
          myCorpus <- tm_map(myCorpus,
                             removePunctuation)
          # remove numbers
          myCorpus <- tm_map(myCorpus,
                             removeNumbers)
          # remove URLs
          removeURL <- function(x){
            sub("http[[:alnum:]]*", "", x)
          }
          myCorpus <- tm_map(myCorpus,
                             content_transformer(removeURL))
          myCorpus <- tm_map(myCorpus,
                             function(x) removeWords(x,
                                                     stopwords("english")))
          myCorpus <- tm_map(myCorpus,
                             stripWhitespace)
          # Return the text corpus
          return(myCorpus)
        }
      
      @
      
    \subsubsection{Create Summary Dataframe}
      We will now create a dataframe that will only contain full and complete summary information. This dataframe will be used to create a global wordcloud.
      <<Remove Summary Information that is null from set, eval=TRUE, echo=TRUE>>=
        my_df.full_summary <- my_df[!(is.na(my_df$summary) | my_df$summary==""), ]
      @


      <<Summary splitting, eval=TRUE, echo=FALSE>>=
        #use this function if you do not have enough ram to be able to covert all the information into a matrix
        spec = c(set.1 = .1,
                 set.2 = .1,
                 set.3 = .1,
                 set.4 = .1,
                 set.5 = .1,
                 set.6 = .1,
                 set.7 = .1,
                 set.8 = .1,
                 set.9 = .1,
                 set.10 = .1)
        
        g = sample(
          cut(
            seq(nrow(my_df.full_summary)),
            nrow(my_df.full_summary)*cumsum(c(0,spec)),
            labels = names(spec)
            )
          )
        
        summarySplits = split(my_df.full_summary, g)
      @
      
    \subsubsection{DocumentTextMatrix Creation}
      We will now create a Document Text Matrix by initially building our corpus, using our buildCorpus function. From there we can then create our Document Text Matrix objects with the end result being a dataframe with words and their frequency of use.

      <<Construct DocumentTermMatrix then create dataframe with word frequency, eval=FALSE>>=
        my_df.AllSummary<-as.data.frame(my_df.full_summary$summary)
        my_df.AllSummary.corpus <- buildCorpus(my_df.AllSummary)
        AllSummary.dtm <- TermDocumentMatrix(my_df.AllSummary.corpus)
        AllSummary.m <- as.matrix(AllSummary.dtm)
        AllSummary.v <- sort(rowSums(AllSummary.m),
                             decreasing=TRUE)
        AllSummary.d <- data.frame(word = names(AllSummary.v),
                                   freq=AllSummary.v)
      @
      
      <<If computer doesnt have enough ram, run this code block, eval=FALSE, echo=FALSE>>=
        #This codeblock only uses a 10% sample of the data, if you are struggling to run the codeblock above! This method will require about 5GB of RAM
        my_df.AllSummary<-as.data.frame(summarySplits$set.1$summary)
        my_df.AllSummary.corpus <- buildCorpus(my_df.AllSummary)
        AllSummary.dtm <- TermDocumentMatrix(my_df.AllSummary.corpus)
        AllSummary.m <- as.matrix(AllSummary.dtm)
        AllSummary.v <- sort(rowSums(AllSummary.m),
                             decreasing=TRUE)
        AllSummary.d <- data.frame(word = names(AllSummary.v),
                                   freq=AllSummary.v)
      @
    \subsubsection{Global Wordcloud Creation}
      Once we have acquired our Word Frequency Matrix, we can then make use of wordcloud2 to show an amazing figure of most commonly used words. Wordcloud2 is more special than wordcloud as it allows us to specify a figPath, that we can map the shape of our output to be like. In this example, as we are showing words from Summaries from all countries globally, my figPath shall make our output look like the globe!
      <<Global wordcloudplot, fig.show='hold', echo=TRUE, eval=FALSE>>=
        wordcloud2(AllSummary.d,
                   figPath = "Assets/world.png",
                   size = 1.5,
                   color = "random-light")
      @
      
      \begin{figure}
        \centering
        \includegraphics[width=1\textwidth]{coverImage}
          \caption{Map of the world, with most frequently used words, used to describe terrorist Incidents.}
          \label{fig:GlobalWordcloud}
      \end{figure}
      
      \clearpage
      
      \pagebreak
      
    \subsubsection{UK Subset Data}
      Now we will analyse the data of UK Terror Incidents. To do this, we will need to create a dataframe which only contains incidents that have occurred in the United Kingdom. From referring to the GTD reference's on their website, the country code for the United Kingdom is 603. We will now create a dataset that only contains incidents from the United Kingdom.
      <<Create UK only Dataset, eval=TRUE, echo=TRUE>>=
        my_df.UK.AllGroups <- my_df[my_df$country==603,]
      @
      
    \subsubsection{UK - Create Summary Dataframe}
      Now we have this data singled out, we can go ahead and start to create a dataframe that only has UK Summary information. We will once again need to make sure that this data has no null values and  is a full dataframe with Summary information.\par This time, instead of comparing records and manually replacing them with null entries, we can make use of the na.omit function that omits null entries in a dataframe. 
      <<Create a subset of UK summary informaction, echo=TRUE, eval=FALSE>>=
        my_df.UK.AllGroups.Summary <- my_df.UK.AllGroups$summary
        my_df.UK.AllGroups.Summary = na.omit(my_df.UK.AllGroups.Summary)
      @
      
    \subsubsection{UK - DocumentTextMatrix Creation}
      We can now once again build our Document Text Matrix using the same format bellow. The only difference between this code and before is we wish to save our data in new variable names, specific to the United Kingdom.
      <<Build Corpus and DTM for UK Summary Information, echo=TRUE, eval=FALSE>>=
        uk.corpus <- buildCorpus(my_df.UK.AllGroups.Summary$summary)
        uk.dtm <- TermDocumentMatrix(uk.corpus)
        uk.m <- as.matrix(uk.dtm)
        uk.v <- sort(rowSums(uk.m),
                     decreasing=TRUE)
        uk.d <- data.frame(word = names(uk.v),
                           freq=uk.v)
      @
      
    \subsubsection{UK Wordcloud Creation}
      Finally, we can create our wordcloud. This time, I've decided to map our wordcloud to a figPath of the United Kingdom. 
      <<Wordcloud of UK common incident Words, eval=FALSE, echo=TRUE, dev='png'>>=
      wordcloud2(uk.d,
                 figPath = "Assets/uk.png",
                 size = 1.5,
                 color = "random-light")
      @
      
      \begin{figure}
        \centering
        \includegraphics[width=1\textwidth]{ukSummaryPlot}
          \caption{Map of the world, with most frequently used words, used to describe terrorist Incidents In the UK.}
          \label{fig:UKWordcloud}
      \end{figure}
      
      \clearpage
      
      \subsubsection{Similarities and Differences between Wordclouds}
      
        \begin{figure}
          \centering
          \begin{subfigure}[b]{0.45\textwidth}
              \includegraphics[width=\textwidth]{coverImage}
              \caption{Global Wordcloud}
              \label{fig:compareGlobal}
          \end{subfigure}
          \begin{subfigure}[b]{0.45\textwidth}
              \includegraphics[width=\textwidth]{ukSummaryPlot}
              \caption{UK Wordcloud}
              \label{fig:compareUK}
          \end{subfigure}
        \end{figure}
        
        
        The repeat of the word 'Fire' in both wordclouds seemed interesting to me. It may be because, as we noticed earlier, due to Armed Assault being the second most common terrorist attack type. It may also be because the word 'Fire' can have multiple uses, such as to describe a blaze, or to use a rifle.\par An interesting difference I noticed is that the word 'bomb' is one of the most popular words used to describe terrorist incidents globally, compared to the UK which uses the plural 'bombs' more frequently. This may suggest that the use of more than one explosive device is more common in UK terror attacks vs the terror attacks that happen on a global scale.
      
      
\section{Random Forest and Predictions}
\label{sec:RFPredictions}

\small
<<AllGroup Known and Unknown Dataset creation, echo=TRUE, eval=FALSE>>=
  my_df.AllGroups <- subset(my_df,
                            select=c(
                              "gname",
                              "region",
                              "nkill",
                              "attacktype1",
                              "success",
                              "weaptype1",
                              "iday",
                              "iyear",
                              "imonth",
                              "targtype1")
                            )
  
  my_df.AllGroups$gname <- factor(my_df.AllGroups$gname)
  
  
  my_df.AllGroups$gname <- fct_lump(my_df.AllGroups$gname,
                                    n = 30)
  
  my_df.AllGroups$gname <- str_replace_all(my_df.AllGroups$gname,
                                           "[^[:alnum:]]",
                                           " ")
  
  my_df.AllGroups=na.omit(my_df.AllGroups)
  
  my_df.AllGroups.Known <- my_df.AllGroups[!(my_df.AllGroups$gname=="Unknown"), ]
  
  my_df.AllGroups.Unknown <- my_df.AllGroups[(my_df.AllGroups$gname=="Unknown"), ]
  
  my_df.AllGroups.Known$gname <-factor(my_df.AllGroups.Known$gname)
  my_df.AllGroups.Known$nkill <-as.numeric(my_df.AllGroups.Known$nkill)
  my_df.AllGroups.Known$region <-factor(my_df.AllGroups.Known$region)
  my_df.AllGroups.Known$success <-factor(my_df.AllGroups.Known$success)
  my_df.AllGroups.Known$attacktype1 <-factor(my_df.AllGroups.Known$attacktype1)
  my_df.AllGroups.Known$weaptype1 <-factor(my_df.AllGroups.Known$weaptype1)
  
  
  my_df.AllGroups.Unknown$gname <-factor(my_df.AllGroups.Unknown$gname)
  my_df.AllGroups.Unknown$nkill <-as.numeric(my_df.AllGroups.Unknown$nkill)
  my_df.AllGroups.Unknown$region <-factor(my_df.AllGroups.Unknown$region)
  my_df.AllGroups.Unknown$success <-factor(my_df.AllGroups.Unknown$success)
  my_df.AllGroups.Unknown$attacktype1 <-factor(my_df.AllGroups.Unknown$attacktype1)
  my_df.AllGroups.Unknown$weaptype1 <-factor(my_df.AllGroups.Unknown$weaptype1)

@
\normalsize
<<Count group attack occurences and plot, echo=TRUE, eval=FALSE>>=
  my_df.AllGroups.freq <- count(my_df$gname)
  my_df.AllGroups.freq <- my_df.AllGroups.freq[!(my_df.AllGroups.freq$x =="Unknown"), ]
  my_df.number.of.groups <- my_df.AllGroups.freq[order(my_df.AllGroups.freq$freq,
                                                       decreasing = T),]
  my_df.group.barplot <- ggplot(data=my_df.number.of.groups[1:10,],
                                aes(
                                  x=x,
                                  y=freq
                                  )
                                ) 
  my_df.group.barplot <- my_df.group.barplot + 
    geom_bar(stat="identity")
  my_df.group.barplot <- my_df.group.barplot + 
    coord_flip()
  my_df.group.barplot <- my_df.group.barplot + 
    theme_igray()
  my_df.group.barplot <- my_df.group.barplot + 
    scale_colour_tableau()
  my_df.group.barplot <- my_df.group.barplot + 
    ggtitle("Global Terrorist Group Attacks")
  my_df.group.barplot <- my_df.group.barplot + 
    xlab("Terorist Group (Name)")
  my_df.group.barplot <- my_df.group.barplot + 
    ylab("Quantity of Incidents (incidents)")
@

<<Global group watermark,eval=TRUE,echo=FALSE>>=
  my_df.group.barplot <- my_df.group.barplot + 
    annotation_custom(xmin=-Inf,
                      ymin=-Inf,
                      xmax=Inf,
                      ymax=Inf,
                      rasterGrob(
                        watermark
                        )
                      )
@

<<Global group plot,eval=TRUE,echo=FALSE>>=
  my_df.group.barplot
@



\small
<<Count UK group attack occurences, echo=TRUE, eval=FALSE>>=
  my_df.UK.AllGroups.freq <-count(my_df.UK.AllGroups$gname)
  my_df.UK.AllGroups.freq <-my_df.UK.AllGroups.freq[!(my_df.UK.AllGroups.freq$x=="Unknown"), ]
  my_df.uk.number.of.groups <-my_df.UK.AllGroups.freq[order(my_df.UK.AllGroups.freq$freq,
                                                             decreasing = T),]
@
\normalsize

<<Make Attack Occurences plot,eval=TRUE,echo=TRUE>>=
  my_df.uk.group.barplot<- ggplot(data=my_df.uk.number.of.groups[1:10,],
                                    aes(
                                      x=x,
                                      y=freq
                                      )
                                    ) 
  my_df.uk.group.barplot <- my_df.uk.group.barplot +
    geom_bar(stat="identity") 
  my_df.uk.group.barplot <- my_df.uk.group.barplot +
    coord_flip()
  my_df.uk.group.barplot <- my_df.uk.group.barplot +
    theme_igray()
  my_df.uk.group.barplot <- my_df.uk.group.barplot +
    scale_colour_tableau()
  my_df.uk.group.barplot <- my_df.uk.group.barplot +
    ggtitle("United Kingdom Terrorist Group Attacks")
  my_df.uk.group.barplot <- my_df.uk.group.barplot +
    xlab("Terorist Group (Name)")
  my_df.uk.group.barplot <- my_df.uk.group.barplot +
    ylab("Quantity of Incidents (incidents)")
@



<<UK GROUP PLOT WATERMARK,eval=TRUE,echo=FALSE>>=
  my_df.uk.group.barplot <- my_df.uk.group.barplot + annotation_custom(xmin=-Inf,
                                                                       ymin=-Inf,
                                                                       xmax=Inf,
                                                                       ymax=Inf,
                                                                       rasterGrob(
                                                                         watermark
                                                                         )
                                                                       )

@

<<Uk Group plot,eval=TRUE,echo=FALSE>>=
  my_df.uk.group.barplot
@





<<Create Training and Testing Set of Group data, , echo=TRUE, eval=FALSE>>=
  inTrain <- createDataPartition(y=my_df.AllGroups.Known$gname,
                                 p=.7,
                                 list=FALSE)
  group.data.training <- my_df.AllGroups.Known[inTrain,]
  group.data.testing <- my_df.AllGroups.Known[-inTrain,]
@




<<Number of trees Effect Random Forest Accuracy, echo=TRUE, eval=FALSE>>=
  trees.ntrees <- 100
  # create dataframe to store accuracies and corresponding number of trees
  trees.df <- data.frame(NTrees=as.numeric(),
                         Accuracy=as.numeric())
  # fit randomForest 10 times, and store the results
  # vary number of trees in every iteration
  
  
  for (i in 1:10){
    trees.RFModel <- drandomForest(gname ~ .,
                                   data=group.data.training,
                                   mtry=4,
                                   importance=TRUE,
                                   na.action=na.omit,
                                   completeModel=TRUE,
                                   nExecutor = nExecutor,
                                   ntree = trees.ntrees,
                                   xtest=group.data.testing[,-1],
                                   ytest=as.data.frame(group.data.testing[,1])
                                   )
    # Test the RF model for this run
    trees.preds <- levels(group.data.training[,1])[trees.RFModel$test$predicted]
    # compute accuracy
    trees.auc <- (sum(trees.preds == group.data.testing[,1])/nrow(group.data.testing))*100
    trees.df <- rbind(trees.df,
                      data.frame(
                        NTrees=trees.ntrees,
                        Accuracy=trees.auc
                        )
                      )
    trees.ntrees <- trees.ntrees + 100
  }# end for loop

@

<<Plot Random Forest Amount of Trees, echo=TRUE, eval=FALSE>>=
  trees.accuracy.plot <- ggplot(trees.df,
                                aes(
                                  x=NTrees,
                                  y=Accuracy
                                  )
                                )
  trees.accuracy.plot <- trees.accuracy.plot +
    geom_line() + geom_point()
  trees.accuracy.plot <- trees.accuracy.plot +
    xlim(100,trees.ntrees)
  trees.accuracy.plot <- trees.accuracy.plot +
    theme_calc()
  trees.accuracy.plot <- trees.accuracy.plot +
    scale_color_calc()
  trees.accuracy.plot <- trees.accuracy.plot +
    ggtitle("Number of Trees vs Model Accuracy")
  trees.accuracy.plot <- trees.accuracy.plot +
    xlab("NTrees (Number of Trees)")
  trees.accuracy.plot <- trees.accuracy.plot +
    ylab("Accuracy (percentage %)")
@

<<Trees plot Watermark,eval=TRUE,echo=FALSE>>=
trees.accuracy.plot <- trees.accuracy.plot + annotation_custom(xmin=-Inf,
                                                                 ymin=-Inf,
                                                                 xmax=Inf,
                                                                 ymax=Inf,
                                                                 rasterGrob(
                                                                   watermark
                                                                   )
                                                                 )
@

<<Trees plot,eval=TRUE,echo=FALSE>>=
  trees.accuracy.plot
@




<<Number of mtry Effect Random Forest Accuracy, echo=TRUE, eval=FALSE>>=
  mtry.mtrys <- 1
  # create dataframe to store accuracies and corresponding number of trees
  mtry.df <- data.frame(NTrees=as.numeric(),
                        Accuracy=as.numeric())
  # fit randomForest 10 times, and store the results
  # vary number of trees in every iteration
  
  
  for (i in 1:5){
    mtry.RFModel <- drandomForest(gname ~ .,
                                  data=group.data.training,
                                  mtry=mtry.mtrys,
                                  importance=TRUE,
                                  na.action=na.omit,
                                  completeModel=TRUE,
                                  nExecutor = nExecutor,
                                  ntree = 100,
                                  xtest=group.data.testing[,-1],
                                  ytest=as.data.frame(group.data.testing[,1]))
    # Test the RF model for this run
    mtry.preds <- levels(group.data.training[,1])[mtry.RFModel$test$predicted]
    # compute accuracy
    mtry.auc <- (sum(mtry.preds == group.data.testing[,1])/nrow(group.data.testing))*100
    mtry.df <- rbind(mtry.df,
                     data.frame(
                       Mtrys=mtry.mtrys,
                       Accuracy=mtry.auc
                       )
                     )
    mtry.mtrys <- mtry.mtrys + 1
  }# end for loop

@

<<Plot Random Forest Amount of MTRYS, echo=TRUE, eval=FALSE>>=
  mtry.accuracy.plot <- ggplot(mtry.df,
                               aes(x=Mtrys,
                                   y=Accuracy))
  mtry.accuracy.plot <- mtry.accuracy.plot + geom_line() + geom_point()
  mtry.accuracy.plot <- mtry.accuracy.plot + xlim(1,mtry.mtrys)
  mtry.accuracy.plot <- mtry.accuracy.plot + theme_calc()
  mtry.accuracy.plot <- mtry.accuracy.plot + scale_color_calc()
  mtry.accuracy.plot <- mtry.accuracy.plot + ggtitle("Number of Mtrys vs Model Accuracy")
  mtry.accuracy.plot <- mtry.accuracy.plot + xlab("Mtrys (Number of Mtrys)")
  mtry.accuracy.plot <- mtry.accuracy.plot + ylab("Accuracy (percentage %)")
@

<<mtry plot watermark,eval=TRUE,echo=FALSE>>=
  mtry.accuracy.plot <- mtry.accuracy.plot + annotation_custom(xmin=-Inf,
                                                               ymin=-Inf,
                                                               xmax=Inf,
                                                               ymax=Inf,
                                                               rasterGrob(
                                                                 watermark
                                                                 )
                                                               )
@

<<mtry plot plot,eval=TRUE,echo=FALSE>>=
  mtry.accuracy.plot
@





<<Create Final Prediction Random Forrest, echo=TRUE, eval=FALSE>>=
  Final.RFModel <- drandomForest(gname ~ .,
                                 data=group.data.training,
                                 mtry=3,
                                 importance=TRUE,
                                 na.action=na.omit,
                                 completeModel=TRUE,
                                 nExecutor = nExecutor,
                                 ntree = 200,
                                 xtest=group.data.testing[,-1],
                                 ytest=as.data.frame(group.data.testing[,1]))
  
  Final.preds <- levels(group.data.training[,1])[Final.RFModel$test$predicted]
  # compute accuracy
  Final.auc <- (sum(Final.preds == group.data.testing[,1])/nrow(group.data.testing))*100
@

<<Final Prediction Model Accuracy Output, echo=TRUE, eval=FALSE>>=
  cat("Final model prediction accuracy:", Final.auc)
  @
  
  
  <<Predict Unknown Attacks using Final Prediction Model, echo=TRUE, eval=FALSE>>=
  predictedUnknowns <- predict.drandomForest(Final.RFModel,
                                             my_df.AllGroups.Unknown[,-1])
  my_df.AllGroups.Unknown.Predicted <- cbind(gname=predictedUnknowns,
                                             my_df.AllGroups.Unknown[,-1])
@


<<Create Bar Plot of all Predicted Terrorist Groups excl Other catagory, echo=TRUE, eval=FALSE>>=
  my_df.predictedUnknowns.freq <- count(my_df.AllGroups.Unknown.Predicted$gname)
  my_df.predictedUnknowns.freq <- my_df.predictedUnknowns.freq[!(my_df.predictedUnknowns.freq$x =="Other"), ]
  my_df.predictedUnknowns.freq <- my_df.predictedUnknowns.freq[order(my_df.predictedUnknowns.freq$freq,
                                                                     decreasing = T),]
  my_df.predictedUnknowns.barplot<-ggplot(data=my_df.predictedUnknowns.freq,
                                          aes(
                                            x=x,
                                            y=freq
                                            )
                                          ) 
  my_df.predictedUnknowns.barplot <- my_df.predictedUnknowns.barplot + geom_bar(stat="identity") 
  my_df.predictedUnknowns.barplot <- my_df.predictedUnknowns.barplot + coord_flip()
  my_df.predictedUnknowns.barplot <- my_df.predictedUnknowns.barplot + theme_igray()
  my_df.predictedUnknowns.barplot <- my_df.predictedUnknowns.barplot + scale_colour_tableau()
  my_df.predictedUnknowns.barplot <- my_df.predictedUnknowns.barplot + ggtitle("Predicted Terrorist Groups and corrisponding Attacks")
  my_df.predictedUnknowns.barplot <- my_df.predictedUnknowns.barplot + xlab("Terorist Group (Name)")
  my_df.predictedUnknowns.barplot <- my_df.predictedUnknowns.barplot + ylab("Quantity of Predicted Incidents (incidents)")
@

<<Predicted Terrorist Watermark plot,eval=TRUE, echo=FALSE>>=
my_df.predictedUnknowns.barplot <- my_df.predictedUnknowns.barplot + annotation_custom(xmin=-Inf,
                                                                                         ymin=-Inf,
                                                                                         xmax=Inf,
                                                                                         ymax=Inf,
                                                                                         rasterGrob(
                                                                                           watermark
                                                                                           )
                                                                                         )
@

<<Predicted Terrorist plot,eval=TRUE,echo=FALSE>>=
  my_df.predictedUnknowns.barplot
@



<<Output Final Model Results, echo=FALSE, eval=FALSE>>=
  cat("Quantity of Unknown Terrorist attacks predicted:",
      nrow(my_df.AllGroups.Unknown.Predicted),
      "attacks.")
  cat("Quantity of Unknown Terrorist attacks predicted (Removing Other):",
      nrow(my_df.AllGroups.Unknown.Predicted[!(my_df.AllGroups.Unknown.Predicted$gname =="Other"), ]),
      "attacks.")
@

\end{document}